PART 2 (Test your implementation to confirm that it indeed distributes workload across the connected
services.):

I opened up the servers in seprate command prompts:
- python cracker_service.py 5000
- python cracker_service.py 5001
- python cracker_service.py 5002

Then, I run the client on the terminal:
- python client.py 5000,5001,5002 cce6b3fb87d8237167f1c5dec15c3133 4

By seeing the incoming http post requests (on each the servers' command prompts), I can conclude that the work is distributed across the servers.

PART 3 (Test your
implementation by manually killing a service (e.g., via Ctrl-C) while running and ensure that the
client continues to explore the search space and does not miss finding the password.):

I opened up the servers in seprate command prompts:
- python cracker_service.py 5000
- python cracker_service.py 5001
- python cracker_service.py 5002
- python cracker_service.py 5003
- python cracker_service.py 5004

Then, I run the client on the terminal:
- python client.py 5000,5001,5002,5003,5004 cce6b3fb87d8237167f1c5dec15c3133 4

Then, I quickly opened up the server's command prompt with the port numbers of 5001, 5002, 5003 and 5004 and click ctrl + c (killing them). After that, I found out the client was still able to get the unhashed password from the last running server.

PART 4:

As I increased the number of servers, I found out that the runtime is decreasing. However, if I keep increasing the maximum length of the word, the runtime will increase as shown in the Scaling Figures.pdf file.

PART 5 (Testing the caching support.):

I opened up the servers:
- python cracker_service.py 5000

Then, I run the client:
- python client.py 5000 06d80eb0c50b49a509b49f2424e8c805 10

Next, I rerun the client:
- python client.py 5000 06d80eb0c50b49a509b49f2424e8c805 10

By finding out the rerun time of the client is smaller (around 455 seconds) than the original run time of the client (around 507 seconds), I can conclude that the server supports caching.

